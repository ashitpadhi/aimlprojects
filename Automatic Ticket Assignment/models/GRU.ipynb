{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitcapstoneaimlmarchgroup2bnlppipenvfbe1d65e7d0d4715bbf9a458a8daab1e",
   "display_name": "Python 3.7.3 64-bit ('capstone_aimlmarchgroup2_b_nlp': pipenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install additional dependencies \n",
    "!pip install spacy_langdetect\n",
    "!pip install contractions\n",
    "!pip install pycountry\n",
    "!pip install textblob\n",
    "!pip install openpyxl\n",
    "\n",
    "!pip install keras_preprocessing\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package stopwords to /Users/vinay/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nUsing TensorFlow backend.\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas_profiling as pp\n",
    "\n",
    "#Importing libraries for text pre-processing\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from contractions import contractions_dict\n",
    "import unicodedata\n",
    "from typing import Dict, List\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Dropout, Bidirectional, TimeDistributed\n",
    "from keras.layers import Activation, Concatenate, SpatialDropout1D, Input, Lambda, Flatten\n",
    "from keras.callbacks import EarlyStopping \n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Embedding, Flatten, Reshape\n",
    "from keras.layers import Concatenate, concatenate\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "project_path: ../data/\n"
    }
   ],
   "source": [
    "env = \"local\" # colab\n",
    "project_path_local = \"../data/\"\n",
    "project_path_colab = \"/content/drive/My Drive/AIML 2019 GreatLearning/Capstone Project NLP/POC\"\n",
    "project_path = project_path_local if env == \"local\" else project_path_colab\n",
    "print(f\"project_path: {project_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_target_variables(row, count=3, original_target_col=\"Assignment group\", count_col=\"tmp_target_count\"):\n",
    "        if row[count_col] <= count:\n",
    "            return \"OTHER\"\n",
    "        else:\n",
    "            return row[original_target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_orig = pd.read_excel(f\"{project_path}/Input Data Synthetic CleanedV3.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(5291, 14)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en = df_en_orig[df_en_orig['lang_textblob']=='en']\n",
    "df_en.reset_index(inplace=True)\n",
    "df_en[\"tmp_target_count\"] = df_en.groupby([\"Assignment group\"])[\"Description\"].transform(\"count\") \n",
    "for index, row in df_en.iterrows():\n",
    "        df_en.loc[index, \"target1\"] = combine_target_variables(row)\n",
    "df_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "MAX_WORDS: 85\nSHORT_DESC_MAX_WORDS: 11\n"
    }
   ],
   "source": [
    "### Paramters\n",
    "df_en['description_cleaned_wc'] = df_en['description_cleaned'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df_en['short_description_cleaned_wc'] = df_en['short_description_cleaned'].apply(lambda x: len(str(x).split(\" \")))\n",
    "MAX_WORDS = int(np.percentile(df_en['description_cleaned_wc'], 95))  ## based on 95 percentile\n",
    "SHORT_DESC_MAX_WORDS = int(np.percentile(df_en['short_description_cleaned_wc'], 95))  ## based on 95 percentile\n",
    "VALIDATION_SPLIT = 0.2 \n",
    "print(f\"MAX_WORDS: {MAX_WORDS}\")\n",
    "print(f\"SHORT_DESC_MAX_WORDS: {SHORT_DESC_MAX_WORDS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(df_en.description_cleaned)\n",
    "tokenizer.fit_on_texts(df_en.short_description_cleaned.apply(lambda x: f\"{x}\"))\n",
    "\n",
    "word_counts = tokenizer.word_counts\n",
    "word_docs = tokenizer.word_docs\n",
    "word_index = tokenizer.word_index\n",
    "index_word = tokenizer.index_word\n",
    "document_count = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "data.shape (5291, 85)\ndata[:2,:] [[ 406    6  119  142   90   73   52    6   73   18    5  189    6   33\n    52  295  163   53   33    9  116    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0]\n [  66  296  104  296    1  297   31  544 3476  189  261    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0]]\n"
    }
   ],
   "source": [
    "no_of_descriptions = df_en.description_cleaned.size\n",
    "data_shape = (no_of_descriptions, MAX_WORDS)\n",
    "data = np.zeros(data_shape, dtype=np.int64)\n",
    "print(\"data.shape\", data.shape)\n",
    "\n",
    "\n",
    "for description_i, description in enumerate(df_en.description_cleaned.to_list()):\n",
    "    # print(f\"{description_i+1} of {no_of_descriptions}\")\n",
    "    for word_i, word in enumerate(text_to_word_sequence(description)):\n",
    "        encoded_word = word_index[word]\n",
    "        if word_i >= MAX_WORDS:\n",
    "            break\n",
    "        elif word_i < MAX_WORDS:\n",
    "            # attempt to update data only if \n",
    "            # sentence_i < MAX_SENTS and word_i < MAX_SENT_LENGTH\n",
    "            data[description_i][word_i] = encoded_word\n",
    "\n",
    "print(\"data[:2,:]\", data[:2,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "short_data.shape (5291, 11)\nshort_data[:2,:] [[33  9  0  0  0  0  0  0  0  0  0]\n [31  0  0  0  0  0  0  0  0  0  0]]\nlogin issue\noutlook\n"
    }
   ],
   "source": [
    "no_of_short_descriptions = df_en.short_description_cleaned.size\n",
    "short_data_shape = (no_of_short_descriptions, SHORT_DESC_MAX_WORDS)\n",
    "short_data = np.zeros(short_data_shape, dtype=np.int64)\n",
    "print(\"short_data.shape\", short_data.shape)\n",
    "\n",
    "\n",
    "for description_i, description in enumerate(df_en.short_description_cleaned.apply(lambda x: f\"{x}\").to_list()):\n",
    "    # print(f\"{description_i+1} of {no_of_short_descriptions}\")\n",
    "    for word_i, word in enumerate(text_to_word_sequence(description)):\n",
    "        encoded_word = word_index[word]\n",
    "        if word_i >= SHORT_DESC_MAX_WORDS:\n",
    "            break\n",
    "        elif word_i < SHORT_DESC_MAX_WORDS:\n",
    "            # attempt to update short_data only if \n",
    "            # sentence_i < MAX_SENTS and word_i < MAX_SENT_LENGTH\n",
    "            short_data[description_i][word_i] = encoded_word\n",
    "\n",
    "print(\"short_data[:2,:]\", short_data[:2,:])\n",
    "print(index_word[33], index_word[9])\n",
    "print(index_word[31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "TARGET_LEN: 55\n['GRP_0' 'GRP_1' 'GRP_3' 'GRP_4' 'GRP_5' 'GRP_8' 'GRP_6' 'GRP_10' 'GRP_9'\n 'GRP_11' 'GRP_14' 'GRP_15' 'GRP_17' 'GRP_18' 'GRP_2' 'GRP_19' 'GRP_20'\n 'GRP_21' 'GRP_25' 'GRP_13' 'GRP_16' 'GRP_26' 'GRP_27' 'GRP_28' 'GRP_29'\n 'GRP_30' 'GRP_31' 'GRP_22' 'GRP_24' 'GRP_7' 'GRP_12' 'GRP_34' 'OTHER'\n 'GRP_36' 'GRP_37' 'GRP_33' 'GRP_39' 'GRP_40' 'GRP_41' 'GRP_43' 'GRP_44'\n 'GRP_45' 'GRP_46' 'GRP_47' 'GRP_23' 'GRP_50' 'GRP_42' 'GRP_51' 'GRP_52'\n 'GRP_53' 'GRP_55' 'GRP_59' 'GRP_60' 'GRP_62' 'GRP_65']\n"
    }
   ],
   "source": [
    "# getting unique labels in given data\n",
    "labels = pd.get_dummies(df_en['target1']).values \n",
    "\n",
    "TARGET_LEN = len(df_en.target1.unique())\n",
    "print(f\"TARGET_LEN: {TARGET_LEN}\")\n",
    "print(df_en.target1.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "x_train.shape:  (4232, 85)   y_train.shape: (4232, 55)\nx_test.shape:  (1059, 85)   y_test.shape:  (1059, 55)\nx_train_short.shape:  (4232, 11)   y_train_short.shape: (4232, 55)\nx_test_short.shape:  (1059, 11)   y_test_short.shape:  (1059, 55)\n"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size = VALIDATION_SPLIT, random_state=9) \n",
    "x_train_short, x_test_short, y_train_short, y_test_short = train_test_split(short_data, labels, test_size = VALIDATION_SPLIT, random_state=9) \n",
    "print(\"x_train.shape: \", x_train.shape, \"  y_train.shape:\", y_train.shape)\n",
    "print(\"x_test.shape: \", x_test.shape, \"  y_test.shape: \", y_test.shape) \n",
    "\n",
    "print(\"x_train_short.shape: \", x_train_short.shape, \"  y_train_short.shape:\", y_train_short.shape)\n",
    "print(\"x_test_short.shape: \", x_test_short.shape, \"  y_test_short.shape: \", y_test_short.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 85)           0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            (None, 11)           0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 85, 150)      1440150     input_1[0][0]                    \n__________________________________________________________________________________________________\nembedding_2 (Embedding)         (None, 11, 150)      1440150     input_2[0][0]                    \n__________________________________________________________________________________________________\ngru_1 (GRU)                     (None, 150)          135450      embedding_1[0][0]                \n__________________________________________________________________________________________________\ngru_2 (GRU)                     (None, 150)          135450      embedding_2[0][0]                \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 300)          0           gru_1[0][0]                      \n                                                                 gru_2[0][0]                      \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256)          77056       concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 55)           14135       dropout_1[0][0]                  \n==================================================================================================\nTotal params: 3,242,391\nTrainable params: 3,242,391\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "### trying with regularizing embedding\n",
    "\n",
    "LSTM_DIM = MAX_WORDS # 128\n",
    "OUTPUT_LEN = 150 # embedding_dimentations\n",
    "vocab_size = len(tokenizer.word_index.keys()) + 1\n",
    "\n",
    "description_encoder_inputs = Input(shape=(MAX_WORDS,))\n",
    "\n",
    "# x1 = Reshape((400,))(description_encoder_inputs)\n",
    "x2 = Embedding(\n",
    "        output_dim=OUTPUT_LEN, \n",
    "        input_dim=vocab_size, \n",
    "        input_length=MAX_WORDS, \n",
    "        embeddings_regularizer=keras.regularizers.l2(.001))(description_encoder_inputs)\n",
    "\n",
    "description_encoder = GRU(OUTPUT_LEN, dropout=0.25, recurrent_dropout=0.25) \n",
    "description_encoder_outputs = description_encoder(x2) # description_state_h, description_state_c\n",
    "\n",
    "\n",
    "short_description_encoder_inputs = Input(shape=(SHORT_DESC_MAX_WORDS,))\n",
    "\n",
    "# sx1 = Reshape((400,))(short_description_encoder_inputs)\n",
    "sx2 = Embedding(\n",
    "        output_dim=OUTPUT_LEN, \n",
    "        input_dim=vocab_size, \n",
    "        input_length=SHORT_DESC_MAX_WORDS, \n",
    "        embeddings_regularizer=keras.regularizers.l2(.001))(short_description_encoder_inputs)\n",
    "\n",
    "short_description_encoder = GRU(OUTPUT_LEN, dropout=0.25, recurrent_dropout=0.25) \n",
    "short_description_encoder_outputs = short_description_encoder(sx2) # description_state_h, description_state_c\n",
    "\n",
    "combined_context_vector = concatenate([description_encoder_outputs, short_description_encoder_outputs])\n",
    "\n",
    "d1 = Dense(units = 256, activation='relu')(combined_context_vector) # combined_context_vector\n",
    "d2 = Dropout(0.2)(d1)\n",
    "predictions = Dense(TARGET_LEN, activation='softmax')(d2) \n",
    "\n",
    "gru_model = Model(inputs=[description_encoder_inputs, short_description_encoder_inputs], outputs=predictions)\n",
    "gru_model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "print(gru_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training...\nTrain on 3385 samples, validate on 847 samples\nEpoch 1/5\n3385/3385 [==============================] - 37s 11ms/step - loss: 115.1554 - accuracy: 0.4682 - val_loss: 2.7832 - val_accuracy: 0.4545\n\nEpoch 00001: saving model to saved_models/gru-01-0.45.hdf5\nEpoch 2/5\n3385/3385 [==============================] - 38s 11ms/step - loss: 2.3342 - accuracy: 0.4789 - val_loss: 2.9104 - val_accuracy: 0.4675\n\nEpoch 00002: saving model to saved_models/gru-02-0.47.hdf5\nEpoch 3/5\n3385/3385 [==============================] - 35s 10ms/step - loss: 2.1067 - accuracy: 0.5288 - val_loss: 2.7176 - val_accuracy: 0.5124\n\nEpoch 00003: saving model to saved_models/gru-03-0.51.hdf5\nEpoch 4/5\n3385/3385 [==============================] - 34s 10ms/step - loss: 1.8999 - accuracy: 0.5533 - val_loss: 2.7238 - val_accuracy: 0.5171\n\nEpoch 00004: saving model to saved_models/gru-04-0.52.hdf5\nEpoch 5/5\n3385/3385 [==============================] - 57s 17ms/step - loss: 1.7668 - accuracy: 0.5669 - val_loss: 2.7675 - val_accuracy: 0.5325\n\nEpoch 00005: saving model to saved_models/gru-05-0.53.hdf5\nCPU times: user 5min 45s, sys: 1min 28s, total: 7min 13s\nWall time: 3min 28s\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x13f55f128>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# batch_size = 32\n",
    "# model.fit(x_train_re, y_train, epochs=5, batch_size=batch_size, verbose=2, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "# filepath = \"saved-model-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "# filepath = \"saved-model-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "filepath = \"saved_models/gru-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=False, mode='auto')\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "gru_model.fit([x_train, x_train_short], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=VALIDATION_SPLIT,\n",
    "          callbacks=[checkpoint]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1059/1059 [==============================] - 3s 3ms/step\nTest loss: 2.7474799847355196\nTest accuracy: 0.5599622130393982\nCPU times: user 4.43 s, sys: 882 ms, total: 5.31 s\nWall time: 2.99 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "# evaluating model on validation data set\n",
    " \n",
    "loss, acc = gru_model.evaluate([x_test, x_test_short], y_test, batch_size=batch_size)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1059/1059 [==============================] - 4s 4ms/step\ny_pred_class: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
    }
   ],
   "source": [
    "y_pred = gru_model.predict([x_test, x_test_short], verbose=1)\n",
    "y_pred_index = np.argmax(y_pred, axis=1)\n",
    "y_pred_class = (y_pred == y_pred.max(axis=1, keepdims=True)).astype(int)\n",
    "print(f\"y_pred_class: {y_pred_class[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "y_pred_class:  [[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\ny_test:  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n              precision    recall  f1-score   support\n\n       GRP_0       0.62      0.98      0.76       511\n       GRP_1       0.00      0.00      0.00         6\n       GRP_3       0.00      0.00      0.00        10\n       GRP_4       0.00      0.00      0.00         5\n       GRP_5       0.17      0.48      0.25        29\n       GRP_8       0.24      0.43      0.30        28\n       GRP_6       0.00      0.00      0.00        17\n      GRP_10       0.00      0.00      0.00         5\n       GRP_9       0.00      0.00      0.00        19\n      GRP_11       0.00      0.00      0.00         4\n      GRP_14       0.00      0.00      0.00        14\n      GRP_15       0.00      0.00      0.00        36\n      GRP_17       0.00      0.00      0.00        40\n      GRP_18       0.00      0.00      0.00         7\n       GRP_2       0.00      0.00      0.00         7\n      GRP_19       0.00      0.00      0.00         6\n      GRP_20       0.00      0.00      0.00         5\n      GRP_21       0.00      0.00      0.00         4\n      GRP_25       0.00      0.00      0.00        12\n      GRP_13       0.00      0.00      0.00        10\n      GRP_16       0.00      0.00      0.00         1\n      GRP_26       0.00      0.00      0.00         9\n      GRP_27       0.00      0.00      0.00        15\n      GRP_28       0.00      0.00      0.00        28\n      GRP_29       0.00      0.00      0.00         2\n      GRP_30       0.00      0.00      0.00         4\n      GRP_31       0.00      0.00      0.00         7\n      GRP_22       0.00      0.00      0.00        12\n      GRP_24       0.00      0.00      0.00         4\n       GRP_7       0.00      0.00      0.00         1\n      GRP_12       0.00      0.00      0.00         1\n      GRP_34       0.00      0.00      0.00        18\n       OTHER       0.00      0.00      0.00         7\n      GRP_36       0.00      0.00      0.00         7\n      GRP_37       0.00      0.00      0.00         1\n      GRP_33       0.00      0.00      0.00         0\n      GRP_39       0.00      0.00      0.00         3\n      GRP_40       0.00      0.00      0.00         4\n      GRP_41       0.00      0.00      0.00         1\n      GRP_43       0.00      0.00      0.00         2\n      GRP_44       0.00      0.00      0.00        18\n      GRP_45       0.00      0.00      0.00         4\n      GRP_46       0.00      0.00      0.00         1\n      GRP_47       0.00      0.00      0.00         2\n      GRP_23       0.00      0.00      0.00         3\n      GRP_50       0.00      0.00      0.00         2\n      GRP_42       0.00      0.00      0.00         1\n      GRP_51       0.67      0.14      0.24        14\n      GRP_52       0.00      0.00      0.00         1\n      GRP_53       0.00      0.00      0.00         8\n      GRP_55       0.00      0.00      0.00         0\n      GRP_59       0.00      0.00      0.00         9\n      GRP_60       0.75      0.72      0.73        81\n      GRP_62       0.23      0.42      0.29        12\n      GRP_65       0.00      0.00      0.00         1\n\n   micro avg       0.56      0.56      0.56      1059\n   macro avg       0.05      0.06      0.05      1059\nweighted avg       0.38      0.56      0.44      1059\n samples avg       0.56      0.56      0.56      1059\n\n"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"y_pred_class: \", y_pred_class[:1])\n",
    "print(\"y_test: \", y_test[:1])\n",
    "print(classification_report(y_test, y_pred_class, target_names=df_en.target1.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}