{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitcapstoneaimlmarchgroup2bnlppipenvfbe1d65e7d0d4715bbf9a458a8daab1e",
   "display_name": "Python 3.7.3 64-bit ('capstone_aimlmarchgroup2_b_nlp': pipenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Requirement already satisfied: spacy_langdetect in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (0.1.2)\nRequirement already satisfied: pytest in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from spacy_langdetect) (5.3.5)\nRequirement already satisfied: langdetect==1.0.7 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from spacy_langdetect) (1.0.7)\nRequirement already satisfied: more-itertools>=4.0.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from pytest->spacy_langdetect) (8.2.0)\nRequirement already satisfied: pluggy<1.0,>=0.12 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from pytest->spacy_langdetect) (0.13.1)\nRequirement already satisfied: packaging in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from pytest->spacy_langdetect) (20.1)\nRequirement already satisfied: wcwidth in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from pytest->spacy_langdetect) (0.1.8)\nRequirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from pytest->spacy_langdetect) (1.5.0)\nRequirement already satisfied: py>=1.5.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from pytest->spacy_langdetect) (1.8.1)\nRequirement already satisfied: attrs>=17.4.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from pytest->spacy_langdetect) (19.3.0)\nRequirement already satisfied: six in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from langdetect==1.0.7->spacy_langdetect) (1.14.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from packaging->pytest->spacy_langdetect) (2.4.6)\nRequirement already satisfied: zipp>=0.5 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->spacy_langdetect) (2.2.0)\nRequirement already satisfied: contractions in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (0.0.24)\nRequirement already satisfied: textsearch in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from contractions) (0.0.17)\nRequirement already satisfied: Unidecode in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from textsearch->contractions) (1.1.1)\nRequirement already satisfied: pyahocorasick in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from textsearch->contractions) (1.4.0)\nRequirement already satisfied: pycountry in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (19.8.18)\nRequirement already satisfied: textblob in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (0.15.3)\nRequirement already satisfied: nltk>=3.1 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from textblob) (3.4.5)\nRequirement already satisfied: six in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.14.0)\nRequirement already satisfied: openpyxl in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (3.0.3)\nRequirement already satisfied: jdcal in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from openpyxl) (1.4.1)\nRequirement already satisfied: et-xmlfile in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from openpyxl) (1.0.1)\nRequirement already satisfied: keras_preprocessing in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (1.1.0)\nRequirement already satisfied: numpy>=1.9.1 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from keras_preprocessing) (1.18.1)\nRequirement already satisfied: six>=1.9.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from keras_preprocessing) (1.14.0)\nRequirement already satisfied: keras in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (2.3.1)\nRequirement already satisfied: keras-applications>=1.0.6 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from keras) (1.0.8)\nRequirement already satisfied: pyyaml in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from keras) (5.3)\nRequirement already satisfied: scipy>=0.14 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from keras) (1.4.1)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from keras) (1.1.0)\nRequirement already satisfied: six>=1.9.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from keras) (1.14.0)\nRequirement already satisfied: numpy>=1.9.1 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from keras) (1.18.1)\nRequirement already satisfied: h5py in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from keras) (2.10.0)\nRequirement already satisfied: tensorflow in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (0.34.2)\nRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (1.4.1)\nRequirement already satisfied: grpcio>=1.8.6 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (1.27.2)\nRequirement already satisfied: numpy<2.0,>=1.16.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (1.18.1)\nRequirement already satisfied: astor>=0.6.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (0.8.1)\nRequirement already satisfied: wrapt>=1.11.1 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (1.11.2)\nRequirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (2.1.0)\nRequirement already satisfied: google-pasta>=0.1.6 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (0.1.8)\nRequirement already satisfied: gast==0.2.2 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (0.2.2)\nRequirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (2.1.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (3.1.0)\nRequirement already satisfied: absl-py>=0.7.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (0.9.0)\nRequirement already satisfied: termcolor>=1.1.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: protobuf>=3.8.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (3.11.3)\nRequirement already satisfied: six>=1.12.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (1.14.0)\nRequirement already satisfied: keras-applications>=1.0.8 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (1.0.8)\nRequirement already satisfied: keras-preprocessing>=1.1.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.22.0)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.11.2)\nRequirement already satisfied: setuptools>=41.0.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (45.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\nRequirement already satisfied: h5py in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\nRequirement already satisfied: idna<2.9,>=2.5 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.25.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0.0)\nRequirement already satisfied: rsa<4.1,>=3.1.4 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\nRequirement already satisfied: pyasn1>=0.1.3 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\nRequirement already satisfied: sklearn in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (0.0)\nRequirement already satisfied: scikit-learn in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from sklearn) (0.22.2)\nRequirement already satisfied: numpy>=1.11.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.18.1)\nRequirement already satisfied: joblib>=0.11 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.14.1)\nRequirement already satisfied: scipy>=0.17.0 in /Users/vinay/.local/share/virtualenvs/capstone_aimlmarchgroup2_b_nlp-nyVBED4V/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.4.1)\n"
    }
   ],
   "source": [
    "# install additional dependencies \n",
    "!pip install spacy_langdetect\n",
    "!pip install contractions\n",
    "!pip install pycountry\n",
    "!pip install textblob\n",
    "!pip install openpyxl\n",
    "\n",
    "!pip install keras_preprocessing\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package stopwords to /Users/vinay/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nUsing TensorFlow backend.\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas_profiling as pp\n",
    "\n",
    "#Importing libraries for text pre-processing\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from contractions import contractions_dict\n",
    "import unicodedata\n",
    "from typing import Dict, List\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Dropout, Bidirectional, TimeDistributed\n",
    "from keras.layers import Activation, Concatenate, SpatialDropout1D, Input, Lambda, Flatten\n",
    "from keras.callbacks import EarlyStopping \n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Embedding, Flatten, Reshape\n",
    "from keras.layers import Concatenate, concatenate\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "project_path: ../data/\n"
    }
   ],
   "source": [
    "env = \"local\" # colab\n",
    "project_path_local = \"../data/\"\n",
    "project_path_colab = \"/content/drive/My Drive/AIML 2019 GreatLearning/Capstone Project NLP/POC\"\n",
    "project_path = project_path_local if env == \"local\" else project_path_colab\n",
    "print(f\"project_path: {project_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_target_variables(row, count=3, original_target_col=\"Assignment group\", count_col=\"tmp_target_count\"):\n",
    "        if row[count_col] <= count:\n",
    "            return \"OTHER\"\n",
    "        else:\n",
    "            return row[original_target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_orig = pd.read_excel(f\"{project_path}/Input Data Synthetic CleanedV3.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(5291, 14)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en = df_en_orig[df_en_orig['lang_textblob']=='en']\n",
    "df_en.reset_index(inplace=True)\n",
    "df_en[\"tmp_target_count\"] = df_en.groupby([\"Assignment group\"])[\"Description\"].transform(\"count\") \n",
    "for index, row in df_en.iterrows():\n",
    "        df_en.loc[index, \"target1\"] = combine_target_variables(row)\n",
    "df_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "MAX_WORDS: 85\nSHORT_DESC_MAX_WORDS: 11\n"
    }
   ],
   "source": [
    "### Paramters\n",
    "df_en['description_cleaned_wc'] = df_en['description_cleaned'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df_en['short_description_cleaned_wc'] = df_en['short_description_cleaned'].apply(lambda x: len(str(x).split(\" \")))\n",
    "MAX_WORDS = int(np.percentile(df_en['description_cleaned_wc'], 95))  ## based on 95 percentile\n",
    "SHORT_DESC_MAX_WORDS = int(np.percentile(df_en['short_description_cleaned_wc'], 95))  ## based on 95 percentile\n",
    "VALIDATION_SPLIT = 0.2 \n",
    "print(f\"MAX_WORDS: {MAX_WORDS}\")\n",
    "print(f\"SHORT_DESC_MAX_WORDS: {SHORT_DESC_MAX_WORDS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(df_en.description_cleaned)\n",
    "tokenizer.fit_on_texts(df_en.short_description_cleaned.apply(lambda x: f\"{x}\"))\n",
    "\n",
    "word_counts = tokenizer.word_counts\n",
    "word_docs = tokenizer.word_docs\n",
    "word_index = tokenizer.word_index\n",
    "index_word = tokenizer.index_word\n",
    "document_count = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "data.shape (5291, 85)\ndata[:2,:] [[ 406    6  119  142   90   73   52    6   73   18    5  189    6   33\n    52  295  163   53   33    9  116    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0]\n [  66  296  104  296    1  297   31  544 3476  189  261    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0]]\n"
    }
   ],
   "source": [
    "no_of_descriptions = df_en.description_cleaned.size\n",
    "data_shape = (no_of_descriptions, MAX_WORDS)\n",
    "data = np.zeros(data_shape, dtype=np.int64)\n",
    "print(\"data.shape\", data.shape)\n",
    "\n",
    "\n",
    "for description_i, description in enumerate(df_en.description_cleaned.to_list()):\n",
    "    # print(f\"{description_i+1} of {no_of_descriptions}\")\n",
    "    for word_i, word in enumerate(text_to_word_sequence(description)):\n",
    "        encoded_word = word_index[word]\n",
    "        if word_i >= MAX_WORDS:\n",
    "            break\n",
    "        elif word_i < MAX_WORDS:\n",
    "            # attempt to update data only if \n",
    "            # sentence_i < MAX_SENTS and word_i < MAX_SENT_LENGTH\n",
    "            data[description_i][word_i] = encoded_word\n",
    "\n",
    "print(\"data[:2,:]\", data[:2,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "short_data.shape (5291, 11)\nshort_data[:2,:] [[33  9  0  0  0  0  0  0  0  0  0]\n [31  0  0  0  0  0  0  0  0  0  0]]\nlogin issue\noutlook\n"
    }
   ],
   "source": [
    "no_of_short_descriptions = df_en.short_description_cleaned.size\n",
    "short_data_shape = (no_of_short_descriptions, SHORT_DESC_MAX_WORDS)\n",
    "short_data = np.zeros(short_data_shape, dtype=np.int64)\n",
    "print(\"short_data.shape\", short_data.shape)\n",
    "\n",
    "\n",
    "for description_i, description in enumerate(df_en.short_description_cleaned.apply(lambda x: f\"{x}\").to_list()):\n",
    "    # print(f\"{description_i+1} of {no_of_short_descriptions}\")\n",
    "    for word_i, word in enumerate(text_to_word_sequence(description)):\n",
    "        encoded_word = word_index[word]\n",
    "        if word_i >= SHORT_DESC_MAX_WORDS:\n",
    "            break\n",
    "        elif word_i < SHORT_DESC_MAX_WORDS:\n",
    "            # attempt to update short_data only if \n",
    "            # sentence_i < MAX_SENTS and word_i < MAX_SENT_LENGTH\n",
    "            short_data[description_i][word_i] = encoded_word\n",
    "\n",
    "print(\"short_data[:2,:]\", short_data[:2,:])\n",
    "print(index_word[33], index_word[9])\n",
    "print(index_word[31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "TARGET_LEN: 55\n['GRP_0' 'GRP_1' 'GRP_3' 'GRP_4' 'GRP_5' 'GRP_8' 'GRP_6' 'GRP_10' 'GRP_9'\n 'GRP_11' 'GRP_14' 'GRP_15' 'GRP_17' 'GRP_18' 'GRP_2' 'GRP_19' 'GRP_20'\n 'GRP_21' 'GRP_25' 'GRP_13' 'GRP_16' 'GRP_26' 'GRP_27' 'GRP_28' 'GRP_29'\n 'GRP_30' 'GRP_31' 'GRP_22' 'GRP_24' 'GRP_7' 'GRP_12' 'GRP_34' 'OTHER'\n 'GRP_36' 'GRP_37' 'GRP_33' 'GRP_39' 'GRP_40' 'GRP_41' 'GRP_43' 'GRP_44'\n 'GRP_45' 'GRP_46' 'GRP_47' 'GRP_23' 'GRP_50' 'GRP_42' 'GRP_51' 'GRP_52'\n 'GRP_53' 'GRP_55' 'GRP_59' 'GRP_60' 'GRP_62' 'GRP_65']\n"
    }
   ],
   "source": [
    "# getting unique labels in given data\n",
    "labels = pd.get_dummies(df_en['target1']).values \n",
    "\n",
    "TARGET_LEN = len(df_en.target1.unique())\n",
    "print(f\"TARGET_LEN: {TARGET_LEN}\")\n",
    "print(df_en.target1.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "x_train.shape:  (4232, 85)   y_train.shape: (4232, 55)\nx_test.shape:  (1059, 85)   y_test.shape:  (1059, 55)\nx_train_short.shape:  (4232, 11)   y_train_short.shape: (4232, 55)\nx_test_short.shape:  (1059, 11)   y_test_short.shape:  (1059, 55)\n"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size = VALIDATION_SPLIT, random_state=9) \n",
    "x_train_short, x_test_short, y_train_short, y_test_short = train_test_split(short_data, labels, test_size = VALIDATION_SPLIT, random_state=9) \n",
    "print(\"x_train.shape: \", x_train.shape, \"  y_train.shape:\", y_train.shape)\n",
    "print(\"x_test.shape: \", x_test.shape, \"  y_test.shape: \", y_test.shape) \n",
    "\n",
    "print(\"x_train_short.shape: \", x_train_short.shape, \"  y_train_short.shape:\", y_train_short.shape)\n",
    "print(\"x_test_short.shape: \", x_test_short.shape, \"  y_test_short.shape: \", y_test_short.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Loaded 400000 word vectors.\n"
    },
    {
     "data": {
      "text/plain": "(9601, 100)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open(f'{project_path}glove.6B.100d.txt')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix for words in vocab\n",
    "vocab_size = len(tokenizer.word_index.keys()) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 85)           0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            (None, 11)           0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 85, 100)      960100      input_1[0][0]                    \n__________________________________________________________________________________________________\nembedding_2 (Embedding)         (None, 11, 100)      960100      input_2[0][0]                    \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, 200)          120600      embedding_1[0][0]                \n__________________________________________________________________________________________________\nbidirectional_2 (Bidirectional) (None, 200)          120600      embedding_2[0][0]                \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 400)          0           bidirectional_1[0][0]            \n                                                                 bidirectional_2[0][0]            \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256)          102656      concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 55)           14135       dropout_1[0][0]                  \n==================================================================================================\nTotal params: 2,278,191\nTrainable params: 357,991\nNon-trainable params: 1,920,200\n__________________________________________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "### trying with regularizing embedding\n",
    "\n",
    "LSTM_DIM = MAX_WORDS # 128\n",
    "OUTPUT_LEN = 100 # embedding_dimentations\n",
    "vocab_size = len(tokenizer.word_index.keys()) + 1\n",
    "\n",
    "description_encoder_inputs = Input(shape=(MAX_WORDS,))\n",
    "\n",
    "# x1 = Reshape((400,))(description_encoder_inputs)\n",
    "# x2 = Embedding(\n",
    "#         output_dim=OUTPUT_LEN, \n",
    "#         input_dim=vocab_size, \n",
    "#         input_length=MAX_WORDS, \n",
    "#         embeddings_regularizer=keras.regularizers.l2(.001))(description_encoder_inputs)\n",
    "\n",
    "x2 = Embedding(input_dim=vocab_size, \n",
    "              output_dim=OUTPUT_LEN, \n",
    "              weights=[embedding_matrix], \n",
    "              input_length=MAX_WORDS, \n",
    "              trainable=False)(description_encoder_inputs)\n",
    "\n",
    "description_encoder = Bidirectional(GRU(OUTPUT_LEN, dropout=0.25, recurrent_dropout=0.25)) # return_state=True\n",
    "description_encoder_outputs = description_encoder(x2) # description_state_h, description_state_c\n",
    "\n",
    "\n",
    "short_description_encoder_inputs = Input(shape=(SHORT_DESC_MAX_WORDS,))\n",
    "\n",
    "# sx1 = Reshape((400,))(short_description_encoder_inputs)\n",
    "# sx2 = Embedding(\n",
    "#         output_dim=OUTPUT_LEN, \n",
    "#         input_dim=vocab_size, \n",
    "#         input_length=SHORT_DESC_MAX_WORDS, \n",
    "#         embeddings_regularizer=keras.regularizers.l2(.001))(short_description_encoder_inputs)\n",
    "\n",
    "sx2 = Embedding(input_dim=vocab_size, \n",
    "              output_dim=OUTPUT_LEN, \n",
    "              weights=[embedding_matrix], \n",
    "              input_length=SHORT_DESC_MAX_WORDS, \n",
    "              trainable=False)(short_description_encoder_inputs)\n",
    "\n",
    "short_description_encoder = Bidirectional(GRU(OUTPUT_LEN, dropout=0.25, recurrent_dropout=0.25)) # return_state=True\n",
    "short_description_encoder_outputs = short_description_encoder(sx2) # description_state_h, description_state_c\n",
    "\n",
    "combined_context_vector = concatenate([description_encoder_outputs, short_description_encoder_outputs])\n",
    "\n",
    "d1 = Dense(units = 256, activation='relu')(combined_context_vector) # combined_context_vector\n",
    "d2 = Dropout(0.2)(d1)\n",
    "predictions = Dense(TARGET_LEN, activation='softmax')(d2) \n",
    "\n",
    "bidirectional_gru_model = Model(inputs=[description_encoder_inputs, short_description_encoder_inputs], outputs=predictions)\n",
    "bidirectional_gru_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "print(bidirectional_gru_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training...\nTrain on 3385 samples, validate on 847 samples\nEpoch 1/5\n3385/3385 [==============================] - 28s 8ms/step - loss: 22670.3839 - accuracy: 0.4691 - val_loss: 2.3660 - val_accuracy: 0.4923\n\nEpoch 00001: saving model to saved_models/glove_bidirectional-gru-01-0.49.hdf5\nEpoch 2/5\n3385/3385 [==============================] - 27s 8ms/step - loss: 3156.3177 - accuracy: 0.5362 - val_loss: 2.2124 - val_accuracy: 0.5041\n\nEpoch 00002: saving model to saved_models/glove_bidirectional-gru-02-0.50.hdf5\nEpoch 3/5\n3385/3385 [==============================] - 32s 9ms/step - loss: 2.8026 - accuracy: 0.5468 - val_loss: 2.1185 - val_accuracy: 0.5136\n\nEpoch 00003: saving model to saved_models/glove_bidirectional-gru-03-0.51.hdf5\nEpoch 4/5\n3385/3385 [==============================] - 31s 9ms/step - loss: 2.3368 - accuracy: 0.5725 - val_loss: 2.0093 - val_accuracy: 0.5407\n\nEpoch 00004: saving model to saved_models/glove_bidirectional-gru-04-0.54.hdf5\nEpoch 5/5\n3385/3385 [==============================] - 33s 10ms/step - loss: 1.7134 - accuracy: 0.5799 - val_loss: 1.9473 - val_accuracy: 0.5478\n\nEpoch 00005: saving model to saved_models/glove_bidirectional-gru-05-0.55.hdf5\nCPU times: user 4min 14s, sys: 41.2 s, total: 4min 55s\nWall time: 2min 36s\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x140042780>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# batch_size = 32\n",
    "# model.fit(x_train_re, y_train, epochs=5, batch_size=batch_size, verbose=2, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "# filepath = \"saved-model-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "# filepath = \"saved-model-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "filepath = \"saved_models/glove_bidirectional-gru-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=False, mode='auto')\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "bidirectional_gru_model.fit([x_train, x_train_short], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=VALIDATION_SPLIT,\n",
    "          callbacks=[checkpoint]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1059/1059 [==============================] - 2s 2ms/step\nTest loss: 1.9455799880851767\nTest accuracy: 0.5609065294265747\nCPU times: user 3.85 s, sys: 522 ms, total: 4.37 s\nWall time: 2.48 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "# evaluating model on validation data set\n",
    " \n",
    "loss, acc = bidirectional_gru_model.evaluate([x_test, x_test_short], y_test, batch_size=batch_size)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1059/1059 [==============================] - 4s 4ms/step\ny_pred_class: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
    }
   ],
   "source": [
    "y_pred = bidirectional_gru_model.predict([x_test, x_test_short], verbose=1)\n",
    "y_pred_index = np.argmax(y_pred, axis=1)\n",
    "y_pred_class = (y_pred == y_pred.max(axis=1, keepdims=True)).astype(int)\n",
    "print(f\"y_pred_class: {y_pred_class[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "y_pred_class:  [[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\ny_test:  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n              precision    recall  f1-score   support\n\n       GRP_0       0.57      0.97      0.72       511\n       GRP_1       0.00      0.00      0.00         6\n       GRP_3       0.00      0.00      0.00        10\n       GRP_4       0.00      0.00      0.00         5\n       GRP_5       0.25      0.14      0.18        29\n       GRP_8       0.00      0.00      0.00        28\n       GRP_6       0.50      0.12      0.19        17\n      GRP_10       0.00      0.00      0.00         5\n       GRP_9       0.20      0.05      0.08        19\n      GRP_11       0.00      0.00      0.00         4\n      GRP_14       0.00      0.00      0.00        14\n      GRP_15       0.75      0.17      0.27        36\n      GRP_17       0.56      0.38      0.45        40\n      GRP_18       0.00      0.00      0.00         7\n       GRP_2       0.00      0.00      0.00         7\n      GRP_19       0.00      0.00      0.00         6\n      GRP_20       0.00      0.00      0.00         5\n      GRP_21       0.00      0.00      0.00         4\n      GRP_25       0.50      0.08      0.14        12\n      GRP_13       0.00      0.00      0.00        10\n      GRP_16       0.00      0.00      0.00         1\n      GRP_26       0.00      0.00      0.00         9\n      GRP_27       0.20      0.07      0.10        15\n      GRP_28       1.00      0.04      0.07        28\n      GRP_29       0.00      0.00      0.00         2\n      GRP_30       0.00      0.00      0.00         4\n      GRP_31       0.00      0.00      0.00         7\n      GRP_22       0.00      0.00      0.00        12\n      GRP_24       0.00      0.00      0.00         4\n       GRP_7       0.00      0.00      0.00         1\n      GRP_12       0.00      0.00      0.00         1\n      GRP_34       0.22      0.11      0.15        18\n       OTHER       0.00      0.00      0.00         7\n      GRP_36       0.00      0.00      0.00         7\n      GRP_37       0.00      0.00      0.00         1\n      GRP_33       0.00      0.00      0.00         0\n      GRP_39       0.00      0.00      0.00         3\n      GRP_40       0.00      0.00      0.00         4\n      GRP_41       0.00      0.00      0.00         1\n      GRP_43       0.00      0.00      0.00         2\n      GRP_44       0.42      0.56      0.48        18\n      GRP_45       0.00      0.00      0.00         4\n      GRP_46       0.00      0.00      0.00         1\n      GRP_47       0.00      0.00      0.00         2\n      GRP_23       0.00      0.00      0.00         3\n      GRP_50       0.00      0.00      0.00         2\n      GRP_42       0.00      0.00      0.00         1\n      GRP_51       0.20      0.07      0.11        14\n      GRP_52       0.00      0.00      0.00         1\n      GRP_53       0.00      0.00      0.00         8\n      GRP_55       0.00      0.00      0.00         0\n      GRP_59       0.00      0.00      0.00         9\n      GRP_60       0.67      0.69      0.68        81\n      GRP_62       0.00      0.00      0.00        12\n      GRP_65       0.00      0.00      0.00         1\n\n   micro avg       0.56      0.56      0.56      1059\n   macro avg       0.11      0.06      0.07      1059\nweighted avg       0.44      0.56      0.45      1059\n samples avg       0.56      0.56      0.56      1059\n\n"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"y_pred_class: \", y_pred_class[:1])\n",
    "print(\"y_test: \", y_test[:1])\n",
    "print(classification_report(y_test, y_pred_class, target_names=df_en.target1.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}