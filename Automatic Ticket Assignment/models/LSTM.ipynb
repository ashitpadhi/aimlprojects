{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitcapstoneaimlmarchgroup2bnlppipenvfbe1d65e7d0d4715bbf9a458a8daab1e",
   "display_name": "Python 3.7.3 64-bit ('capstone_aimlmarchgroup2_b_nlp': pipenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install additional dependencies \n",
    "!pip install spacy_langdetect\n",
    "!pip install contractions\n",
    "!pip install pycountry\n",
    "!pip install textblob\n",
    "!pip install openpyxl\n",
    "\n",
    "!pip install keras_preprocessing\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package stopwords to /Users/vinay/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nUsing TensorFlow backend.\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas_profiling as pp\n",
    "\n",
    "#Importing libraries for text pre-processing\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from contractions import contractions_dict\n",
    "import unicodedata\n",
    "from typing import Dict, List\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Dropout, Bidirectional, TimeDistributed\n",
    "from keras.layers import Activation, Concatenate, SpatialDropout1D, Input, Lambda, Flatten\n",
    "from keras.callbacks import EarlyStopping \n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Embedding, Flatten, Reshape\n",
    "from keras.layers import Concatenate, concatenate\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "project_path: ../data/\n"
    }
   ],
   "source": [
    "env = \"local\" # colab\n",
    "project_path_local = \"../data/\"\n",
    "project_path_colab = \"/content/drive/My Drive/AIML 2019 GreatLearning/Capstone Project NLP/POC\"\n",
    "project_path = project_path_local if env == \"local\" else project_path_colab\n",
    "print(f\"project_path: {project_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_target_variables(row, count=3, original_target_col=\"Assignment group\", count_col=\"tmp_target_count\"):\n",
    "        if row[count_col] <= count:\n",
    "            return \"OTHER\"\n",
    "        else:\n",
    "            return row[original_target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_orig = pd.read_excel(f\"{project_path}/Input Data Synthetic CleanedV3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(5291, 14)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en = df_en_orig[df_en_orig['lang_textblob']=='en']\n",
    "df_en.reset_index(inplace=True)\n",
    "df_en[\"tmp_target_count\"] = df_en.groupby([\"Assignment group\"])[\"Description\"].transform(\"count\") \n",
    "for index, row in df_en.iterrows():\n",
    "        df_en.loc[index, \"target1\"] = combine_target_variables(row)\n",
    "\n",
    "\n",
    "df_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "df_en['target1']\n GRP_0     2511\nGRP_8      374\nGRP_2      205\nGRP_12     180\nGRP_19     170\nGRP_3      159\nGRP_13     122\nGRP_14     101\nGRP_4       91\nGRP_25      87\nGRP_5       87\nGRP_29      81\nGRP_9       79\nGRP_16      75\nGRP_10      73\nGRP_18      71\nGRP_7       64\nGRP_6       62\nGRP_26      49\nGRP_34      40\nGRP_40      38\nGRP_15      36\nOTHER       35\nGRP_41      35\nGRP_28      31\nGRP_20      31\nGRP_22      29\nGRP_11      27\nGRP_24      27\nGRP_33      26\nGRP_21      26\nGRP_31      24\nGRP_45      23\nGRP_23      20\nGRP_1       20\nGRP_39      15\nGRP_60      14\nGRP_37      14\nGRP_47      13\nGRP_44      12\nGRP_27      12\nGRP_50      10\nGRP_62      10\nGRP_30      10\nGRP_17       9\nGRP_36       9\nGRP_53       8\nGRP_65       8\nGRP_51       7\nGRP_52       6\nGRP_55       6\nGRP_42       5\nGRP_59       5\nGRP_43       5\nGRP_46       4\nName: target1, dtype: int64\n"
    }
   ],
   "source": [
    "print(\"df_en['target1']\\n\", df_en['target1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "MAX_WORDS: 85\nSHORT_DESC_MAX_WORDS: 11\n"
    }
   ],
   "source": [
    "### Paramters\n",
    "df_en['description_cleaned_wc'] = df_en['description_cleaned'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df_en['short_description_cleaned_wc'] = df_en['short_description_cleaned'].apply(lambda x: len(str(x).split(\" \")))\n",
    "MAX_WORDS = int(np.percentile(df_en['description_cleaned_wc'], 95))  ## based on 95 percentile\n",
    "SHORT_DESC_MAX_WORDS = int(np.percentile(df_en['short_description_cleaned_wc'], 95))  ## based on 95 percentile\n",
    "VALIDATION_SPLIT = 0.2 \n",
    "print(f\"MAX_WORDS: {MAX_WORDS}\")\n",
    "print(f\"SHORT_DESC_MAX_WORDS: {SHORT_DESC_MAX_WORDS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(df_en.description_cleaned)\n",
    "tokenizer.fit_on_texts(df_en.short_description_cleaned.apply(lambda x: f\"{x}\"))\n",
    "\n",
    "word_counts = tokenizer.word_counts\n",
    "word_docs = tokenizer.word_docs\n",
    "word_index = tokenizer.word_index\n",
    "index_word = tokenizer.index_word\n",
    "document_count = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "data.shape (5291, 85)\ndata[:2,:] [[ 406    6  119  142   90   73   52    6   73   18    5  189    6   33\n    52  295  163   53   33    9  116    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0]\n [  66  296  104  296    1  297   31  544 3476  189  261    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0]]\n"
    }
   ],
   "source": [
    "no_of_descriptions = df_en.description_cleaned.size\n",
    "data_shape = (no_of_descriptions, MAX_WORDS)\n",
    "data = np.zeros(data_shape, dtype=np.int64)\n",
    "print(\"data.shape\", data.shape)\n",
    "\n",
    "\n",
    "for description_i, description in enumerate(df_en.description_cleaned.to_list()):\n",
    "    # print(f\"{description_i+1} of {no_of_descriptions}\")\n",
    "    for word_i, word in enumerate(text_to_word_sequence(description)):\n",
    "        encoded_word = word_index[word]\n",
    "        if word_i >= MAX_WORDS:\n",
    "            break\n",
    "        elif word_i < MAX_WORDS:\n",
    "            # attempt to update data only if \n",
    "            # sentence_i < MAX_SENTS and word_i < MAX_SENT_LENGTH\n",
    "            data[description_i][word_i] = encoded_word\n",
    "\n",
    "print(\"data[:2,:]\", data[:2,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "short_data.shape (5291, 11)\nshort_data[:2,:] [[33  9  0  0  0  0  0  0  0  0  0]\n [31  0  0  0  0  0  0  0  0  0  0]]\nlogin issue\noutlook\n"
    }
   ],
   "source": [
    "no_of_short_descriptions = df_en.short_description_cleaned.size\n",
    "short_data_shape = (no_of_short_descriptions, SHORT_DESC_MAX_WORDS)\n",
    "short_data = np.zeros(short_data_shape, dtype=np.int64)\n",
    "print(\"short_data.shape\", short_data.shape)\n",
    "\n",
    "\n",
    "for description_i, description in enumerate(df_en.short_description_cleaned.apply(lambda x: f\"{x}\").to_list()):\n",
    "    # print(f\"{description_i+1} of {no_of_short_descriptions}\")\n",
    "    for word_i, word in enumerate(text_to_word_sequence(description)):\n",
    "        encoded_word = word_index[word]\n",
    "        if word_i >= SHORT_DESC_MAX_WORDS:\n",
    "            break\n",
    "        elif word_i < SHORT_DESC_MAX_WORDS:\n",
    "            # attempt to update short_data only if \n",
    "            # sentence_i < MAX_SENTS and word_i < MAX_SENT_LENGTH\n",
    "            short_data[description_i][word_i] = encoded_word\n",
    "\n",
    "print(\"short_data[:2,:]\", short_data[:2,:])\n",
    "print(index_word[33], index_word[9])\n",
    "print(index_word[31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "TARGET_LEN: 55\n['GRP_0' 'GRP_1' 'GRP_3' 'GRP_4' 'GRP_5' 'GRP_8' 'GRP_6' 'GRP_10' 'GRP_9'\n 'GRP_11' 'GRP_14' 'GRP_15' 'GRP_17' 'GRP_18' 'GRP_2' 'GRP_19' 'GRP_20'\n 'GRP_21' 'GRP_25' 'GRP_13' 'GRP_16' 'GRP_26' 'GRP_27' 'GRP_28' 'GRP_29'\n 'GRP_30' 'GRP_31' 'GRP_22' 'GRP_24' 'GRP_7' 'GRP_12' 'GRP_34' 'OTHER'\n 'GRP_36' 'GRP_37' 'GRP_33' 'GRP_39' 'GRP_40' 'GRP_41' 'GRP_43' 'GRP_44'\n 'GRP_45' 'GRP_46' 'GRP_47' 'GRP_23' 'GRP_50' 'GRP_42' 'GRP_51' 'GRP_52'\n 'GRP_53' 'GRP_55' 'GRP_59' 'GRP_60' 'GRP_62' 'GRP_65']\n"
    }
   ],
   "source": [
    "# getting unique labels in given data\n",
    "labels = pd.get_dummies(df_en['target1']).values \n",
    "\n",
    "TARGET_LEN = len(df_en.target1.unique())\n",
    "print(f\"TARGET_LEN: {TARGET_LEN}\")\n",
    "print(df_en.target1.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "x_train.shape:  (4232, 85)   y_train.shape: (4232, 55)\nx_test.shape:  (1059, 85)   y_test.shape:  (1059, 55)\nx_train_short.shape:  (4232, 11)   y_train_short.shape: (4232, 55)\nx_test_short.shape:  (1059, 11)   y_test_short.shape:  (1059, 55)\n"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size = VALIDATION_SPLIT, random_state=9) \n",
    "x_train_short, x_test_short, y_train_short, y_test_short = train_test_split(short_data, labels, test_size = VALIDATION_SPLIT, random_state=9) \n",
    "print(\"x_train.shape: \", x_train.shape, \"  y_train.shape:\", y_train.shape)\n",
    "print(\"x_test.shape: \", x_test.shape, \"  y_test.shape: \", y_test.shape) \n",
    "\n",
    "print(\"x_train_short.shape: \", x_train_short.shape, \"  y_train_short.shape:\", y_train_short.shape)\n",
    "print(\"x_test_short.shape: \", x_test_short.shape, \"  y_test_short.shape: \", y_test_short.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            (None, 85)           0                                            \n__________________________________________________________________________________________________\ninput_4 (InputLayer)            (None, 11)           0                                            \n__________________________________________________________________________________________________\nembedding_3 (Embedding)         (None, 85, 150)      1440150     input_3[0][0]                    \n__________________________________________________________________________________________________\nembedding_4 (Embedding)         (None, 11, 150)      1440150     input_4[0][0]                    \n__________________________________________________________________________________________________\nlstm_3 (LSTM)                   (None, 150)          180600      embedding_3[0][0]                \n__________________________________________________________________________________________________\nlstm_4 (LSTM)                   (None, 150)          180600      embedding_4[0][0]                \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 300)          0           lstm_3[0][0]                     \n                                                                 lstm_4[0][0]                     \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 256)          77056       concatenate_2[0][0]              \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 256)          0           dense_3[0][0]                    \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 55)           14135       dropout_2[0][0]                  \n==================================================================================================\nTotal params: 3,332,691\nTrainable params: 3,332,691\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "### trying with regularizing embedding\n",
    "OUTPUT_LEN = 150 # embedding_dimentations\n",
    "vocab_size = len(tokenizer.word_index.keys()) + 1\n",
    "\n",
    "description_encoder_inputs = Input(shape=(MAX_WORDS,))\n",
    "\n",
    "# x1 = Reshape((400,))(description_encoder_inputs)\n",
    "x2 = Embedding(\n",
    "        output_dim=OUTPUT_LEN, \n",
    "        input_dim=vocab_size, \n",
    "        input_length=MAX_WORDS, \n",
    "        embeddings_regularizer=keras.regularizers.l2(.001))(description_encoder_inputs)\n",
    "\n",
    "description_encoder = LSTM(OUTPUT_LEN, dropout=0.25, recurrent_dropout=0.25) \n",
    "description_encoder_outputs = description_encoder(x2) # description_state_h, description_state_c\n",
    "\n",
    "\n",
    "short_description_encoder_inputs = Input(shape=(SHORT_DESC_MAX_WORDS,))\n",
    "\n",
    "# sx1 = Reshape((400,))(short_description_encoder_inputs)\n",
    "sx2 = Embedding(\n",
    "        output_dim=OUTPUT_LEN, \n",
    "        input_dim=vocab_size, \n",
    "        input_length=SHORT_DESC_MAX_WORDS, \n",
    "        embeddings_regularizer=keras.regularizers.l2(.001))(short_description_encoder_inputs)\n",
    "\n",
    "short_description_encoder = LSTM(OUTPUT_LEN, dropout=0.25, recurrent_dropout=0.25) \n",
    "short_description_encoder_outputs = short_description_encoder(sx2) # description_state_h, description_state_c\n",
    "\n",
    "combined_context_vector = concatenate([description_encoder_outputs, short_description_encoder_outputs])\n",
    "\n",
    "d1 = Dense(units = 256, activation='relu')(combined_context_vector) # combined_context_vector\n",
    "d2 = Dropout(0.2)(d1)\n",
    "predictions = Dense(TARGET_LEN, activation='softmax')(d2) \n",
    "\n",
    "LSTM_model = Model(inputs=[description_encoder_inputs, short_description_encoder_inputs], outputs=predictions)\n",
    "LSTM_model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "print(LSTM_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training...\nTrain on 3385 samples, validate on 847 samples\nEpoch 1/10\n3385/3385 [==============================] - 35s 10ms/step - loss: 3.2937 - accuracy: 0.4815 - val_loss: 2.2783 - val_accuracy: 0.5148\n\nEpoch 00001: saving model to saved_models/lstm-01-0.51.hdf5\nEpoch 2/10\n3385/3385 [==============================] - 33s 10ms/step - loss: 2.1126 - accuracy: 0.5448 - val_loss: 2.2013 - val_accuracy: 0.5242\n\nEpoch 00002: saving model to saved_models/lstm-02-0.52.hdf5\nEpoch 3/10\n3385/3385 [==============================] - 33s 10ms/step - loss: 1.9719 - accuracy: 0.5501 - val_loss: 2.1296 - val_accuracy: 0.5289\n\nEpoch 00003: saving model to saved_models/lstm-03-0.53.hdf5\nEpoch 4/10\n3385/3385 [==============================] - 29s 9ms/step - loss: 1.8945 - accuracy: 0.5613 - val_loss: 2.1475 - val_accuracy: 0.5218\n\nEpoch 00004: saving model to saved_models/lstm-04-0.52.hdf5\nEpoch 5/10\n3385/3385 [==============================] - 34s 10ms/step - loss: 1.8660 - accuracy: 0.5634 - val_loss: 2.1814 - val_accuracy: 0.5159\n\nEpoch 00005: saving model to saved_models/lstm-05-0.52.hdf5\nEpoch 6/10\n3385/3385 [==============================] - 33s 10ms/step - loss: 1.8161 - accuracy: 0.5669 - val_loss: 2.1482 - val_accuracy: 0.5195\n\nEpoch 00006: saving model to saved_models/lstm-06-0.52.hdf5\nEpoch 7/10\n3385/3385 [==============================] - 32s 10ms/step - loss: 1.7882 - accuracy: 0.5775 - val_loss: 2.1153 - val_accuracy: 0.5277\n\nEpoch 00007: saving model to saved_models/lstm-07-0.53.hdf5\nEpoch 8/10\n3385/3385 [==============================] - 33s 10ms/step - loss: 1.7132 - accuracy: 0.5897 - val_loss: 2.1372 - val_accuracy: 0.5230\n\nEpoch 00008: saving model to saved_models/lstm-08-0.52.hdf5\nEpoch 9/10\n3385/3385 [==============================] - 33s 10ms/step - loss: 1.6480 - accuracy: 0.6068 - val_loss: 2.1595 - val_accuracy: 0.5242\n\nEpoch 00009: saving model to saved_models/lstm-09-0.52.hdf5\nEpoch 10/10\n3385/3385 [==============================] - 33s 10ms/step - loss: 1.6054 - accuracy: 0.6142 - val_loss: 2.2199 - val_accuracy: 0.5242\n\nEpoch 00010: saving model to saved_models/lstm-10-0.52.hdf5\nCPU times: user 10min 7s, sys: 2min 25s, total: 12min 33s\nWall time: 5min 38s\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x146352358>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# batch_size = 32\n",
    "# model.fit(x_train_re, y_train, epochs=5, batch_size=batch_size, verbose=2, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "# filepath = \"saved-model-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "# filepath = \"saved-model-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "filepath = \"saved_models/lstm-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=False, mode='auto')\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "LSTM_model.fit([x_train, x_train_short], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=VALIDATION_SPLIT,\n",
    "          callbacks=[checkpoint]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1059/1059 [==============================] - 3s 3ms/step\nTest loss: 2.154598119463754\nTest accuracy: 0.5514636635780334\nCPU times: user 4.75 s, sys: 720 ms, total: 5.47 s\nWall time: 2.67 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "# evaluating model on validation data set\n",
    " \n",
    "loss, acc = LSTM_model.evaluate([x_test, x_test_short], y_test, batch_size=batch_size)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1059/1059 [==============================] - 3s 3ms/step\ny_pred_class: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
    }
   ],
   "source": [
    "y_pred = LSTM_model.predict([x_test, x_test_short], verbose=1)\n",
    "y_pred_index = np.argmax(y_pred, axis=1)\n",
    "y_pred_class = (y_pred == y_pred.max(axis=1, keepdims=True)).astype(int)\n",
    "print(f\"y_pred_class: {y_pred_class[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "y_pred_class:  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\ny_test:  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n              precision    recall  f1-score   support\n\n       GRP_0       0.71      0.90      0.79       511\n       GRP_1       0.00      0.00      0.00         6\n       GRP_3       0.00      0.00      0.00        10\n       GRP_4       0.00      0.00      0.00         5\n       GRP_5       0.29      0.55      0.38        29\n       GRP_8       0.21      0.57      0.31        28\n       GRP_6       0.00      0.00      0.00        17\n      GRP_10       0.00      0.00      0.00         5\n       GRP_9       0.00      0.00      0.00        19\n      GRP_11       0.00      0.00      0.00         4\n      GRP_14       0.00      0.00      0.00        14\n      GRP_15       0.00      0.00      0.00        36\n      GRP_17       0.48      0.30      0.37        40\n      GRP_18       0.00      0.00      0.00         7\n       GRP_2       0.00      0.00      0.00         7\n      GRP_19       0.00      0.00      0.00         6\n      GRP_20       0.00      0.00      0.00         5\n      GRP_21       0.00      0.00      0.00         4\n      GRP_25       0.00      0.00      0.00        12\n      GRP_13       0.00      0.00      0.00        10\n      GRP_16       0.00      0.00      0.00         1\n      GRP_26       0.00      0.00      0.00         9\n      GRP_27       0.11      0.20      0.14        15\n      GRP_28       0.11      0.39      0.18        28\n      GRP_29       0.00      0.00      0.00         2\n      GRP_30       0.00      0.00      0.00         4\n      GRP_31       0.00      0.00      0.00         7\n      GRP_22       0.00      0.00      0.00        12\n      GRP_24       0.00      0.00      0.00         4\n       GRP_7       0.00      0.00      0.00         1\n      GRP_12       0.00      0.00      0.00         1\n      GRP_34       0.00      0.00      0.00        18\n       OTHER       0.00      0.00      0.00         7\n      GRP_36       0.00      0.00      0.00         7\n      GRP_37       0.00      0.00      0.00         1\n      GRP_33       0.00      0.00      0.00         0\n      GRP_39       0.00      0.00      0.00         3\n      GRP_40       0.00      0.00      0.00         4\n      GRP_41       0.00      0.00      0.00         1\n      GRP_43       0.00      0.00      0.00         2\n      GRP_44       0.21      0.17      0.19        18\n      GRP_45       0.00      0.00      0.00         4\n      GRP_46       0.00      0.00      0.00         1\n      GRP_47       0.00      0.00      0.00         2\n      GRP_23       0.00      0.00      0.00         3\n      GRP_50       0.00      0.00      0.00         2\n      GRP_42       0.00      0.00      0.00         1\n      GRP_51       0.00      0.00      0.00        14\n      GRP_52       0.00      0.00      0.00         1\n      GRP_53       0.00      0.00      0.00         8\n      GRP_55       0.00      0.00      0.00         0\n      GRP_59       0.00      0.00      0.00         9\n      GRP_60       0.84      0.72      0.77        81\n      GRP_62       0.09      0.25      0.13        12\n      GRP_65       0.00      0.00      0.00         1\n\n   micro avg       0.55      0.55      0.55      1059\n   macro avg       0.06      0.07      0.06      1059\nweighted avg       0.45      0.55      0.49      1059\n samples avg       0.55      0.55      0.55      1059\n\n"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"y_pred_class: \", y_pred_class[:1])\n",
    "print(\"y_test: \", y_test[:1])\n",
    "print(classification_report(y_test, y_pred_class, target_names=df_en.target1.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}